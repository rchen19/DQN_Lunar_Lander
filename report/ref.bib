
@article{SuttonLearningPredictMethods1988,
  title = {Learning to {{Predict}} by the {{Methods}} of {{Temporal Differences}}},
  volume = {3},
  issn = {0885-6125, 1573-0565},
  doi = {10.1023/A:1022633531479},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction \textendash{} that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  language = {en},
  number = {1},
  journal = {Machine Learning},
  author = {Sutton, Richard S.},
  month = aug,
  year = {1988},
  pages = {9-44},
  file = {/Users/ranchen/Zotero/storage/4IS4E8GT/Sutton - 1988 - Learning to Predict by the Methods of Temporal Dif.pdf;/Users/ranchen/Zotero/storage/EAEBEIN4/A1022633531479.html}
}

@article{vanHasseltDeepReinforcementLearning2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.06461},
  primaryClass = {cs},
  title = {Deep {{Reinforcement Learning}} with {{Double Q}}-Learning},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  journal = {arXiv:1509.06461 [cs]},
  author = {{van Hasselt}, Hado and Guez, Arthur and Silver, David},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ranchen/Zotero/storage/ZVHFHPV5/van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf;/Users/ranchen/Zotero/storage/NRKWHXJZ/1509.html}
}

@article{RoderickImplementingDeepQNetwork2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.07478},
  primaryClass = {cs},
  title = {Implementing the {{Deep Q}}-{{Network}}},
  abstract = {The Deep Q-Network proposed by Mnih et al. [2015] has become a benchmark and building point for much deep reinforcement learning research. However, replicating results for complex systems is often challenging since original scientific publications are not always able to describe in detail every important parameter setting and software engineering solution. In this paper, we present results from our work reproducing the results of the DQN paper. We highlight key areas in the implementation that were not covered in great detail in the original paper to make it easier for researchers to replicate these results, including termination conditions and gradient descent algorithms. Finally, we discuss methods for improving the computational performance and provide our own implementation that is designed to work with a range of domains, and not just the original Arcade Learning Environment [Bellemare et al., 2013].},
  journal = {arXiv:1711.07478 [cs]},
  author = {Roderick, Melrose and MacGlashan, James and Tellex, Stefanie},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ranchen/Zotero/storage/ANM8UQCU/Roderick et al. - 2017 - Implementing the Deep Q-Network.pdf;/Users/ranchen/Zotero/storage/I3YAYXGT/1711.html}
}

@article{WangDuelingNetworkArchitectures2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06581},
  primaryClass = {cs},
  title = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  journal = {arXiv:1511.06581 [cs]},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and {van Hasselt}, Hado and Lanctot, Marc and {de Freitas}, Nando},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ranchen/Zotero/storage/LIG5JWPY/Wang et al. - 2015 - Dueling Network Architectures for Deep Reinforceme.pdf;/Users/ranchen/Zotero/storage/C6QUCN53/1511.html}
}

@article{LillicrapContinuouscontroldeep2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.02971},
  primaryClass = {cs, stat},
  title = {Continuous Control with Deep Reinforcement Learning},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  journal = {arXiv:1509.02971 [cs, stat]},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ranchen/Zotero/storage/R8WYBRBA/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf;/Users/ranchen/Zotero/storage/4I7YDWRX/1509.html}
}

@article{MnihPlayingAtariDeep2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.5602},
  primaryClass = {cs},
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  journal = {arXiv:1312.5602 [cs]},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  month = dec,
  year = {2013},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ranchen/Zotero/storage/9U5G5HJY/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/Users/ranchen/Zotero/storage/YR3KMA2D/1312.html}
}

@article{MnihHumanlevelcontroldeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  volume = {518},
  copyright = {2015 Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  abstract = {The theory of reinforcement learning provides a normative account1, deeply rooted in psychological2 and neuroscientific3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems4,5, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms3. While reinforcement learning agents have achieved some successes in a variety of domains6,7,8, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks9,10,11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games12. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
  language = {en},
  number = {7540},
  journal = {Nature},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  month = feb,
  year = {2015},
  pages = {529-533},
  file = {/Users/ranchen/Zotero/storage/J2JF7RBW/nature14236.html}
}

@article{SchulmanProximalPolicyOptimization2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.06347},
  primaryClass = {cs},
  title = {Proximal {{Policy Optimization Algorithms}}},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  journal = {arXiv:1707.06347 [cs]},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ranchen/Zotero/storage/8H89BIZB/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/Users/ranchen/Zotero/storage/832DDGYI/1707.html}
}

@inproceedings{MnihAsynchronousMethodsDeep2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present as...},
  language = {en},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  month = jun,
  year = {2016},
  pages = {1928-1937},
  file = {/Users/ranchen/Zotero/storage/4CAW9KS9/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf;/Users/ranchen/Zotero/storage/G2RFSB2S/mniha16.html}
}

@article{SchaulPrioritizedExperienceReplay2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.05952},
  primaryClass = {cs},
  title = {Prioritized {{Experience Replay}}},
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  journal = {arXiv:1511.05952 [cs]},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ranchen/Zotero/storage/P4TF9WHN/Schaul et al. - 2015 - Prioritized Experience Replay.pdf;/Users/ranchen/Zotero/storage/9ZE6R2RZ/1511.html}
}


